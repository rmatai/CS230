{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.rdd import RDD\n",
    "# #from pyspark.sql import SQLContext, HiveContext\n",
    "# from pyspark.sql.functions import broadcast\n",
    "# from pyspark.sql.functions import lit, rand, concat\n",
    "# from pyspark.sql.column import Column, _to_java_column, _to_seq\n",
    "import subprocess\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                .config(\"spark.sql.crossJoin.enabled\",\"true\") \\\n",
    "                .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                .config(\"spark.executor.memory\", \"10g\") \\\n",
    "                .config(\"spark.memory.fraction\", \"0.35\") \\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = 20200601\n",
    "\n",
    "viewing_data = spark.sql(\"\"\"\n",
    "                select p.profile_id, case when vertical_group = '--' and vertical_category = 'Anime' then 'Anime'\n",
    "                                    when vertical_group = '--' then 'Other'\n",
    "                                    else vertical_group end as vertical,\n",
    "                        sum(view_secs) as vert_view_secs\n",
    "                from dse.vrt_show_vertical_r v\n",
    "                join dse.figment_profile_title_viewing p on p.show_title_id = v.show_title_id\n",
    "                and p.dateint >= {0}\n",
    "                and v.vertical_order_nbr = 1\n",
    "                and p.playback_country_iso_code = 'DE' \n",
    "                group by 1,2 \n",
    "                having vert_view_secs > 1000 limit 10000\n",
    "            \"\"\".format(today))\n",
    "\n",
    "viewing_data.registerTempTable(\"viewing_data\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE rmatai.profile_viewing_data_DE AS SELECT * from viewing_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"rmatai.profile_viewing_data_FR_new1\")\n",
    "verticals_pivot = df.groupBy(\"profile_id\")\\\n",
    "                           .pivot(\"vertical\")\\\n",
    "                           .agg(F.sum(\"vert_view_secs\"))\n",
    "\n",
    "verticals_pivot1 = verticals_pivot.na.fill(0)\n",
    "verticals_pivot1.registerTempTable(\"verticals_pivot1\")\n",
    "spark.sql(\"CREATE TABLE rmatai.profile_viewing_pivot_FR_new2 AS SELECT * from verticals_pivot1\")\n",
    "verticals_pivot1.show(20, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = spark.table(\"rmatai.profile_viewing_pivot_FR_new2\")\n",
    "\n",
    "sample_data1 = sample_data\n",
    "for col in sample_data.columns:\n",
    "  sample_data1 = sample_data1.withColumnRenamed(col,col.replace(\" \", \"_\"))\n",
    "\n",
    "sample_data2 = sample_data1\n",
    "for col in sample_data1.columns:\n",
    "  sample_data2 = sample_data2.withColumnRenamed(col,col.replace(\"&\", \"\")).withColumnRenamed(col,col.replace(\"(\", \"\"))\n",
    "\n",
    "sample_data3 = sample_data2\n",
    "for col in sample_data2.columns:\n",
    "  sample_data3 = sample_data3.withColumnRenamed(col,col.replace(\"-\", \"\")).withColumnRenamed(col,col.replace(\"/\", \"\"))\n",
    "\n",
    "sample_data4 = sample_data3\n",
    "for col in sample_data3.columns:\n",
    "  sample_data4 = sample_data4.withColumnRenamed(col,col.replace(\")\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data4.registerTempTable(\"sample_data4\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE rmatai.profile_viewing_pivot_FR_new3 as select * from sample_data4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data4.col_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.table(\"rmatai.profile_viewing_pivot_FR_new3\")\n",
    "\n",
    "view_vec = df1.withColumn(\"view_list\", F.array(col_list[1:])).select(\"profile_id\", \"view_list\")\n",
    "view_vec.registerTempTable(\"view_vec\")\n",
    "spark.sql(\"CREATE TABLE rmatai.profile_viewing_vec_FR as select * from view_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE rmatai.profile_viewing_matrix_FR_new as \\\n",
    "select a.profile_id, a.view_list , b.profile_id as connected_profile_id, b.view_list connected_view_list \\\n",
    "from rmatai.profile_viewing_vec_FR a \\\n",
    "full outer join rmatai.profile_viewing_vec_FR b \\\n",
    "on a.profile_id != b.profile_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Compute Cosine similarity between two profile's viewing vectors\n",
    "@udf(\"float\")\n",
    "def cosine_udf(l, l2):\n",
    "    return float(1-spatial.distance.cosine(l, l2))\n",
    "\n",
    "df = spark.table(\"rmatai.profile_viewing_matrix_FR_new\")\n",
    "#df = spark.sql(\"\"\"select * from rmatai.profile_viewing_matrix_DE_new limit 10000\"\"\")\n",
    "result1 = df.select(df.profile_id, df.connected_profile_id, cosine_udf(df.view_list, df.connected_view_list).alias(\"cos_sim\"))\n",
    "#result1.show(20, False)\n",
    "result1.registerTempTable(\"result1\")\n",
    "spark.sql(\"CREATE TABLE rmatai.profile_viewing_cosine_FR_new as select * from result1\")\n",
    "\n",
    "# spark.sql(\"CREATE TABLE rmatai.profile_viewing_cosine_DE as \\\n",
    "# select a.profile_id, a.connected_profile_id, cosine_udf(a.view_list, a.connected_view_list) \\\n",
    "# from rmatai.profile_viewing_matrix_DE a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib histogram\n",
    "plt.hist(result1['cos_sim'], color = 'blue', edgecolor = 'black',\n",
    "         bins = int(20))\n",
    "\n",
    "# seaborn histogram\n",
    "sns.distplot(result1['cos_sim'], hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Distribution of Cosine Similarity')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Number of Edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE rmatai.profile_adjlist as \\\n",
    "select t1.profile_id, t2.profile_id \\\n",
    "from rmatai.profile_verticals_viewing_sample t1 \\\n",
    "inner join rmatai.profile_verticals_viewing_sample t2\\\n",
    "on t1.vertical = t2.vertical\\\n",
    "where t1.profile_id < t2.profile_id\\\n",
    "and t1.vert_view_secs > 2000 and t2.vert_view_secs > 2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "@udf(\"float\")\n",
    "def cosine_udf(l, l2):\n",
    "    return float(1-spatial.distance.cosine(l, l2))\n",
    "\n",
    "df = spark.table(\"rmatai.profile_matrix_sample_FR2\")\n",
    "\n",
    "result1 = df.select(df.profile_id, df.connected_profile_id, cosine_udf(df.view_list, df.connected_view_list))\n",
    "result1.show(20, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Spark 2.3.2 - Python 3 (venv)",
   "language": "python",
   "name": "spark23-python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
