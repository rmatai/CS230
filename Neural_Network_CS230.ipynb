{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.enable_eager_execution ()\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "#Load data from Hive tables, see feature extraction ETL for more details#\n",
    "    START_DATEINT = 20200505\n",
    "    REGION = 'APAC'\n",
    "\n",
    "    load_data_query = '''\n",
    "        SELECT cast(profile_id as string) as profile_id,\n",
    "            cast(account_id as string) as account_id,\n",
    "            region_rollup_desc,\n",
    "            signup_country_iso_code, \n",
    "            sub_tenure_days,\n",
    "            account_total_sec_7d,\n",
    "            account_total_sec_14d,\n",
    "            account_total_sec_28d,\n",
    "            account_days_with_play_7d,\n",
    "            account_days_with_play_14d,\n",
    "            account_days_with_play_28d,\n",
    "            profile_total_sec_7d,\n",
    "            profile_total_sec_14d,\n",
    "            profile_total_sec_28d,\n",
    "            profile_days_with_play_7d,\n",
    "            profile_days_with_play_14d,\n",
    "            profile_days_with_play_28d,\n",
    "            profile_days_with_new_originals_play_7d,\n",
    "            profile_days_with_new_originals_play_14d,\n",
    "            profile_days_with_new_originals_play_28d,\n",
    "            est_tenure_12m\n",
    "        from dse.figment_profile_algo_features \n",
    "        where region_dateint = %i\n",
    "        and region_rollup_desc=  \\'%s\\'\n",
    "        limit 100000\n",
    "        '''\n",
    "\n",
    "    \n",
    "    load_data_query = (load_data_query) % (START_DATEINT,\n",
    "                                           REGION)\n",
    "    features_data = spark.sql(load_data_query)\n",
    "\n",
    "    #Convert the dataframe to Pandas\n",
    "    all_data = features_data.select(\"*\").toPandas()\n",
    "    \n",
    "#   print(all_data.head(10))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for the model\n",
    "#Convert all boolean columns to int\n",
    "def bool_to_int(dataset, column_arr):\n",
    "    dataset[column_arr] = dataset[column_arr].astype('int')\n",
    "    return dataset\n",
    "\n",
    "#one-hot encoder categorical features\n",
    "def one_hot_encoder(dataset, feature):\n",
    "    featureBinarizer = LabelBinarizer().fit(dataset[feature])\n",
    "    feature_encoded = featureBinarizer.transform(dataset[feature])\n",
    "    return feature_encoded\n",
    "\n",
    "#Embedding for show, season and title ids\n",
    "def feature_embed(dataset, feature):\n",
    "    embedding_layer = layers.Embedding(900000081342889666, 100, input_length=32)\n",
    "    feature_embedding = embedding_layer(tf.constant(dataset[feature]))\n",
    "    return feature_embedding\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(dataset):\n",
    "    # create scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit scaler on data\n",
    "    scaler.fit(dataset)\n",
    "    # apply transform\n",
    "    normalized = scaler.transform(dataset)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlation(dataset):\n",
    "    #Analyze the data correlation\n",
    "    C_mat = dataset.corr()\n",
    "    fig = plt.figure(figsize = (15,15))\n",
    "\n",
    "    sb.heatmap(C_mat, vmax = .8, square = True)\n",
    "    plt.savefig('./feature_correlation.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_data()\n",
    "    \n",
    "    #if plot:\n",
    "    # plot the correlation between features\n",
    "analyze_correlation(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_NN_model(data):\n",
    "    #turn off the eager execution in tensorflow\n",
    "    INPUT_LAYER = 25\n",
    "    HIDDEN_LAYER1 = 20\n",
    "    HIDDEN_LAYER2 = 20\n",
    "    HIDDEN_LAYER3 = 20\n",
    "    OUTPUT_LAYER = 1\n",
    "    \n",
    "    ACIVATION_FUNC = 'relu'\n",
    "    OPTIMIZER = 'adam'\n",
    "    METRIC = 'mean_absolute_error'\n",
    "    \n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "    NN_model = Sequential()\n",
    "\n",
    "    # The Input Layer :\n",
    "    NN_model.add(Dense(INPUT_LAYER, kernel_initializer='normal',input_dim = data.shape[1], activation=ACIVATION_FUNC))\n",
    "\n",
    "    # The Hidden Layers :\n",
    "    NN_model.add(Dense(HIDDEN_LAYER1, kernel_initializer='normal',activation=ACIVATION_FUNC))\n",
    "    NN_model.add(Dense(HIDDEN_LAYER2, kernel_initializer='normal',activation=ACIVATION_FUNC))\n",
    "    NN_model.add(Dense(HIDDEN_LAYER3, kernel_initializer='normal',activation=ACIVATION_FUNC))\n",
    " #   NN_model.add(Dense(HIDDEN_LAYER4, kernel_initializer='normal',activation=ACIVATION_FUNC))\n",
    "\n",
    "\n",
    "    # The Output Layer :\n",
    "    NN_model.add(Dense(OUTPUT_LAYER, kernel_initializer='normal',activation=ACIVATION_FUNC))\n",
    "\n",
    "    # Compile the network :\n",
    "    NN_model.compile(loss=METRIC, optimizer=OPTIMIZER, metrics=[METRIC])\n",
    "    NN_model.summary()\n",
    "    return NN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(orig_data, pred_data):\n",
    "    #plot predictions and expected y labels\n",
    "    p = pred_data[:10000]\n",
    "    y = orig_data[:10000]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    orig, = ax.plot(range(y.shape[0]),y,'bs')\n",
    "    pred, = ax.plot(range(y.shape[0]),p, 'g^')\n",
    "    leg2 = ax.legend([orig,pred],['original data','prediction data'], loc='best')\n",
    "    plt.ylabel('28 Day Profile Total Playtime')\n",
    "    plt.xlabel('Netflix Profile')\n",
    "    plt.title('Playtime Predictions')\n",
    "    plt.savefig('./prediction_accuracy.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(plot=True):\n",
    "    # Seed the randomness of the simulation so this outputs the same thing each time\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    EPOCHS = 40\n",
    "    BATCH_SIZE = 1000\n",
    "    VALIDATIION_SPLIT = 0.1\n",
    "    \n",
    "    #load all data for modeling\n",
    "    all_data = load_data()\n",
    "    \n",
    "    #if plot:\n",
    "    # plot the correlation between features\n",
    "    analyze_correlation(all_data)\n",
    "    #The prediction for profile_total_sec_28d, consider log for normal distribution\n",
    "    target = np.log(all_data.profile_total_sec_28d.replace(0,1))\n",
    "    #Prepare data for the model\n",
    "    #Convert all boolean columns to int\n",
    "#     bool_cols = ['is_season_finale', 'is_kids', 'is_original']\n",
    "#     bool_to_int(all_data, bool_cols)\n",
    "    \n",
    "    #Identify continuous features\n",
    "    continuous = ['sub_tenure_days',\n",
    "            'account_total_sec_7d',\n",
    "            'account_total_sec_14d',\n",
    "            'account_total_sec_28d',\n",
    "            'account_days_with_play_7d',\n",
    "            'account_days_with_play_14d',\n",
    "            'account_days_with_play_28d',\n",
    "            'profile_total_sec_7d',\n",
    "            'profile_total_sec_14d',\n",
    "            'profile_total_sec_28d',\n",
    "            'profile_days_with_play_7d',\n",
    "            'profile_days_with_play_14d',\n",
    "            'profile_days_with_play_28d',\n",
    "            'profile_days_with_new_originals_play_7d',\n",
    "            'profile_days_with_new_originals_play_14d',\n",
    "            'profile_days_with_new_originals_play_28d',\n",
    "            'est_tenure_12m']\n",
    "    \n",
    "\n",
    "    dataContinuous = all_data[continuous]\n",
    "    \n",
    "   # one-hot encode the language categorical data \n",
    "    region_onehot = one_hot_encoder(all_data,'region_rollup_desc')\n",
    "    # one-hot encode the country categorical data \n",
    "    cntry_onehot = one_hot_encoder(all_data,'signup_country_iso_code')\n",
    "    \n",
    "    #Embedding for show, season and title ids\n",
    "    profileId_emb = one_hot_encoder(all_data, 'profile_id')\n",
    "    accountId_emb = one_hot_encoder(all_data, 'account_id')\n",
    "    \n",
    "    # Combining all of above transformed data\n",
    "    dataX = np.hstack([dataContinuous, region_onehot, cntry_onehot, profileId_emb, accountId_emb])\n",
    "    print('here')\n",
    "  \n",
    "    #Normalize data\n",
    "    normalized_features = normalize_data(dataX)\n",
    "    \n",
    "    # Split into train and test dataset for 70:30\n",
    "    train_X, test_X, train_y, test_y = train_test_split(normalized_features, target, test_size = 0.30, random_state = 10)\n",
    "    \n",
    "    # Create checkpoint for model training\n",
    "    checkpoint_name = 'Weights.hdf5' \n",
    "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    NN_model = define_NN_model(train_X)\n",
    "    \n",
    "    NN_model.summary()\n",
    "    \n",
    "    #Train the model NN_model\n",
    "\n",
    "    NN_model.fit(train_X, train_y, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split = VALIDATIION_SPLIT,\n",
    "             callbacks=[plot_losses])\n",
    "\n",
    "    #Make predictions on test data\n",
    "    predictions = NN_model.predict(test_X)\n",
    "    print(predictions.shape)\n",
    "    #if plot:\n",
    "    # plot the correlation between features\n",
    "    plot_predictions(test_y, predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPlot(keras.callbacks.Callback):\n",
    "    \n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.accuracy = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracy = []\n",
    "        self.logs = []\n",
    "    \n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.logs.append(logs)\n",
    "        print(self.logs)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        \n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.losses) > 1:\n",
    "            \n",
    "            # Clear the previous plot\n",
    "            clear_output(wait=True)\n",
    "            N = np.arange(0, len(self.losses))\n",
    "            \n",
    "            # You can chose the style of your preference\n",
    "            print(self.val_losses)\n",
    "            print(self.losses)\n",
    "            # print(plt.style.available) to see the available options\n",
    "            plt.style.use(\"seaborn\")\n",
    "            \n",
    "            # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "            #plt.figure()\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.plot(N, self.losses, label = \"train_loss\")\n",
    "            #plt.plot(N, self.accuracy, label = \"train_acc\")\n",
    "            plt.plot(N, self.val_losses, label = \"val_loss\")\n",
    "            #plt.plot(N, self.val_accuracy, label = \"val_acc\")\n",
    "            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(epoch))\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Loss/Accuracy\")\n",
    "            plt.legend()\n",
    "            plt.savefig('./NN_model_losses_plot.png')\n",
    "            plt.show()\n",
    "\n",
    "plot_losses = TrainingPlot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Spark 2.3.2 - Python 3 (venv)",
   "language": "python",
   "name": "spark23-python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
